{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Aman Chulawala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Neural Volume Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0: Transmittance Calculations\n",
    "\n",
    "![calculation](data/figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3. Ray sampling (5 points)\n",
    "\n",
    "|![Image 1](data/1.3-grid.png)|![Image 2](data/1.3-rays.png)|\n",
    "---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4. Point Sampling (5 points)\n",
    "\n",
    "![Image 1](data/1.4_samples_pc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5. Volume Rendering (20 points)\n",
    "\n",
    "\n",
    "|![Image 1](data/part_1.gif)|![Image 2](data/1.5_depth.png)|\n",
    "---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Optimizing a basic implicit volume (10 points)\n",
    "\n",
    "![Image 1](data/part_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Optimizing a Neural Radiance Field (NeRF) (20 points)\n",
    "\n",
    "|Render|![Image](data/part_3_20.gif)|![Image](data/part_3_40.gif)|![Image](data/part_3_60.gif)|![Image](data/part_3_80.gif)|![Image](data/part_3_100.gif)\n",
    "---|---|---|---|---|---\n",
    "|Epoch|20|40|60|80|100|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1. View Dependence (10 points)\n",
    "\n",
    "![Image 1](data/part_4_materials_140.gif)\n",
    "\n",
    "**Discuss the trade-offs between increased view dependence and generalization quality**\n",
    "\n",
    "Trade-offs:\n",
    "\n",
    "- Increased Realism: By considering the viewing direction, the model can produce more lifelike images with accurate reflections and shading.\n",
    "- Risk of Overfitting: If not implemented carefully, the model might overfit to the training images, learning view-specific artifacts that don’t generalize well to unseen views.\n",
    "- Generalization Quality: A model with high view dependence might struggle to generalize to new viewpoints, especially if the training data is limited or lacks variety in viewing angles.\n",
    "\n",
    "To balance these trade-offs, it’s important to have a diverse set of training views and to possibly regularize the model to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Neural Surface Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Sphere Tracing (10pts)\n",
    "\n",
    "![Image 1](data/part_5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Optimizing a Neural SDF (15pts)\n",
    "\n",
    "|![Image 1](data/part_6_input.gif)|![Image 2](data/part_6.gif)|\n",
    "---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7. VolSDF (15 pts)\n",
    "\n",
    "Alpha and beta in the context of Signed Distance Functions (SDFs) and their impact on training and surface reconstruction.\n",
    "\n",
    "**Alpha:**\n",
    "\n",
    "The alpha parameter controls the density (or opacity) of the scene. It determines how much light is absorbed or scattered as it travels through the medium. A higher alpha value results in a denser medium, meaning that light has a harder time passing through it. This corresponds to regions in the scene where objects are more solid or opaque. In volume rendering, a higher alpha value would make the material appear more solid and less transparent. For example, in medical imaging, high alpha values might represent dense tissues like bones.\n",
    "\n",
    "**Beta:**\n",
    "\n",
    "The beta parameter controls the emission of light from the scene. It determines how much light is emitted from each point in the volume. A higher beta value leads to more light being emitted from the scene. This corresponds to regions where the material emits light (e.g., glowing objects, light sources). In volume rendering, high beta values create emissive materials (like neon signs or glowing lava). Low beta values result in non-emissive materials.\n",
    "\n",
    "**How does high beta bias your learned SDF? What about low beta?**\n",
    "\n",
    "When beta is high, the model tends to emphasize emissive regions. It becomes more sensitive to light sources and bright areas. However, this can lead to overfitting if the training data lacks diverse lighting conditions. When beta is low, the model focuses less on emissive regions. It may struggle to capture subtle lighting effects or glow. However, it might generalize better to different lighting conditions.\n",
    "\n",
    "**Would an SDF be easier to train with volume rendering and low beta or high beta? Why?**\n",
    "\n",
    "Training with low beta is generally easier because it reduces the complexity of the problem. The model doesn’t need to learn intricate lighting interactions. However, it might miss out on important emissive features. Training with high beta is more challenging due to increased sensitivity to lighting variations. The model must learn to handle diverse illumination conditions, which can lead to overfitting.\n",
    "\n",
    "**Would you be more likely to learn an accurate surface with high beta or low beta? Why?**\n",
    "\n",
    "For accurate surface reconstruction, low beta is preferable. It encourages the model to focus on geometric details rather than emissive effects. However, it might miss out on fine illumination nuances. While high beta captures emissive features well, it might introduce noise and artifacts. Achieving an accurate surface with high beta requires careful regularization and diverse training data.\n",
    "\n",
    "|Render|![Image](data/part_7_alpha1_beta1.gif)|![Image](data/part_7_alpha10_beta1.gif)|![Image](data/part_7_alpha10_beta005.gif)|\n",
    "---|---|---|---\n",
    "|Geometry|![Image](data/part_7_geometry_alpha1_beta1.gif)|![Image](data/part_7_geometry_alpha10_beta1.gif)|![Image](data/part_7_geometry_alpha10_beta005.gif)|\n",
    "|Alpha|1|10|10|\n",
    "|Beta|1|1|0.05|\n",
    "\n",
    "**Best Results**\n",
    "\n",
    "\n",
    "|![Image 1](data/part_7.gif)|![Image 2](data/part_7_geometry.gif)|\n",
    "---|---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8.2 Fewer Training Views (10 pts)\n",
    "\n",
    "|No. of Views|NeRF Render|SDF Geometry|SDF|\n",
    "---|---|---|---\n",
    "|100|![Image](data/part_8_NeRF_100.gif)|![Image](data/part_8_SDF_geometry_100.gif)|![Image](data/part_8_SDF_100.gif)|\n",
    "|50|![Image](data/part_8_NeRF_50.gif)|![Image](data/part_8_SDF_geometry_50.gif)|![Image](data/part_8_SDF_50.gif)|\n",
    "|20|![Image](data/part_8_NeRF_20.gif)|![Image](data/part_8_SDF_geometry_20.gif)|![Image](data/part_8_SDF_20.gif)|"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
