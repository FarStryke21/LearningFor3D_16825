{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "Aman Chulawala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Exploring Loss Functions\n",
    "**Prompt 1.1:** Voxel Grid\n",
    "\n",
    "|Voxel Grid|![Voxel Source](data/q1/1_voxelsrc.gif)|![Voxel Target](data/q1/1_voxeltgt.gif)|\n",
    "---|---|---\n",
    "\n",
    "**Prompt 1.2:** Point Cloud\n",
    "\n",
    "|Point Cloud|![Point Cloud Source](data/q1/1_pcdsrc.gif)|![Point Cloud Target](data/q1/1_pcdtgt.gif)|\n",
    "---|---|---\n",
    "\n",
    "\n",
    "**Prompt 1.3:** Mesh\n",
    "\n",
    "|Mesh|![Mesh Source](data/q1/1_meshsrc.gif)|![Mesh Target](data/q1/1_meshtgt.gif)|\n",
    "---|---|---\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Reconstruction from 2D\n",
    "**Prompt 2.1:** Voxel\n",
    "\n",
    "|Input Image|Ground Truth Mesh|Prediction|\n",
    "---|---|---\n",
    "|![Example 1 Image](data/q21/gt_img_0.png)|![Example 1 Mesh](data/q21/gt_mesh_0.gif)|![Example 1 Prediction](data/q21/0_vox.gif)|\n",
    "|![Example 2 Image](data/q21/gt_img_20.png)|![Example 2 Mesh](data/q21/gt_mesh_20.gif)|![Example 2 Prediction](data/q21/20_vox.gif)|\n",
    "|![Example 3 Image](data/q21/gt_img_185.png)|![Example 3 Mesh](data/q21/gt_mesh_185.gif)|![Example 3 Prediction](data/q21/185_vox.gif)|\n",
    "\n",
    "----\n",
    "**Prompt 2.2:** Point Cloud\n",
    "\n",
    "|Input Image|Ground Truth Mesh|Prediction|\n",
    "---|---|---\n",
    "|![Example 1 Image](data/q22/gt_img_100.png)|![Example 1 Mesh](data/q22/gt_pcd_100.gif)|![Example 1 Prediction](data/q22/100_point.gif)|\n",
    "|![Example 2 Image](data/q22/gt_img_350.png)|![Example 2 Mesh](data/q22/gt_pcd_350.gif)|![Example 2 Prediction](data/q22/350_point.gif)|\n",
    "|![Example 3 Image](data/q22/gt_img_650.png)|![Example 3 Mesh](data/q22/gt_pcd_650.gif)|![Example 3 Prediction](data/q22/650_point.gif)|\n",
    "\n",
    "-----\n",
    "**Prompt 2.3:** Mesh\n",
    "\n",
    "|Input Image|Ground Truth Mesh|Prediction|\n",
    "---|---|---\n",
    "|![Example 1 Image](data/q23/gt_img_100.png)|![Example 1 Mesh](data/q23/gt_mesh_100.gif)|![Example 1 Prediction](data/q23/100_mesh.gif)|\n",
    "|![Example 2 Image](data/q23/gt_img_400.png)|![Example 2 Mesh](data/q23/gt_mesh_400.gif)|![Example 2 Prediction](data/q23/400_mesh.gif)|\n",
    "|![Example 3 Image](data/q23/gt_img_600.png)|![Example 3 Mesh](data/q23/gt_mesh_600.gif)|![Example 3 Prediction](data/q23/600_mesh.gif)|\n",
    "\n",
    "-----\n",
    "**Prompt 2.4:** Quantitative comparisions\n",
    "|Evaluation Voxel|Evaluation Point|Evaluation Mesh|\n",
    "---|---|---\n",
    "|![Voxel Eval](data/q24/eval_vox.png)|![Point Eval](data/q24/eval_point.png)|![Mesh Eval](data/q24/eval_mesh.png)|\n",
    "\n",
    "**Quantitative Comparison:** The F1 score demonstrates a notable disparity among the three methods, showcasing the remarkable performance of point clouds over voxel grids and meshes. This pronounced superiority of point clouds can be attributed to their inherent flexibility and unparalleled precision in depicting intricate 3D shapes. Unlike voxel grids, which struggle due to their binary representation that often oversimplifies complex forms, point clouds excel in capturing fine details and nuances, thereby achieving higher fidelity in shape representation.\n",
    "\n",
    "Furthermore, while meshes rely on faces to convey shape continuity, they often encounter limitations in accurately representing the intricate geometry of 3D objects. Their reliance on surface approximation may result in inaccuracies, particularly in complex structures where finer details are crucial. Thus, despite their potential in certain applications, meshes fall short compared to point clouds in faithfully capturing the intricacies of 3D shapes.\n",
    "\n",
    "In summary, the superior performance of point clouds in F1 scoring underscores their versatility and precision, making them the preferred choice for tasks requiring accurate representation of 3D shapes, while voxel grids and meshes exhibit limitations that hinder their effectiveness in such scenarios.\n",
    "\n",
    "-----\n",
    "**Prompt 2.5:** Hyperparam Variation\n",
    "\n",
    "I decided to vary the mesh smoothness factor of the loss and observe performance of the mesh model. I explored the impact of varying the smoothing parameter, w_smooth, using values of 0.1, 1, and 2, after 10,000 training iterations due to resource and time limitations. Despite a decrease in F1 scores with increasing w_smooth, the visual appearance of the reconstructed mesh is notably affected. Even though the F1 score changes minimally, any level of smoothing (w_smooth > 0) leads to smoother surface finishes, reducing sharpness. Excessive smoothing negatively impacts performance, likely due to the loss of critical geometric details essential for precise reconstruction.\n",
    "\n",
    "|Input Image|Ground Truth Mesh|Prediction for w=0.1|Prediction for w=1|Prediction for w=2|\n",
    "---|---|---|---|---\n",
    "|![Example 1 Image](data/q25/gt_img_100.png)|![Example 1 Mesh](data/q25/gt_mesh_100.gif)|![Example 1 Prediction](data/q25/wdef/100_mesh.gif)|![Example 1 Prediction](data/q25/w1/100_mesh.gif)|![Example 1 Prediction](data/q25/w2/100_mesh.gif)|\n",
    "|![Example 2 Image](data/q25/gt_img_400.png)|![Example 2 Mesh](data/q25/gt_mesh_400.gif)|![Example 2 Prediction](data/q25/wdef/400_mesh.gif)|![Example 1 Prediction](data/q25/w1/400_mesh.gif)|![Example 1 Prediction](data/q25/w2/400_mesh.gif)|\n",
    "|![Example 3 Image](data/q25/gt_img_600.png)|![Example 3 Mesh](data/q25/gt_mesh_600.gif)|![Example 3 Prediction](data/q25/wdef/600_mesh.gif)|![Example 1 Prediction](data/q25/w1/600_mesh.gif)|![Example 1 Prediction](data/q25/w2/600_mesh.gif)|\n",
    "|||![Example 3 Image](data/q25/eval_mesh_0.png)|![Example 3 Image](data/q25/eval_mesh_1.png)|![Example 3 Image](data/q25/eval_mesh_2.png)|\n",
    "\n",
    "-----\n",
    "**Prompt 2.6:** Model Interpretation\n",
    "\n",
    "![Example 3 Image](data/q26/output_1.png) ![Example 3 Image](data/q26/output_2.png) ![Example 3 Image](data/q26/output.png)\n",
    "\n",
    "In the realm of model evaluation and performance analysis, understanding the significance of various features is crucial for refining and optimizing algorithms. Leveraging the MapExtrackt package, I developed a short function to elucidate the performance of my models by highlighting salient features within images. This approach allows for a deeper understanding of model predictions and provides insights into areas of improvement.\n",
    "\n",
    "By employing MapExtrackt's robust feature extraction capabilities, I can effectively pinpoint important elements within images and assess how well my models capture and interpret them. This function facilitates a comprehensive evaluation process, enabling me to visualize and analyze the strengths and weaknesses of different algorithms. Through this approach, I tried to refine model architectures, fine-tune hyperparameters, and ultimately enhance overall performance in various applications.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Training with entire dataset\n",
    "\n",
    "|Input Image|Ground Truth Mesh|Prediction|\n",
    "---|---|---\n",
    "|![Example 1 Image](data/q33/gt_img_50.png)|![Example 1 Mesh](data/q33/gt_mesh_50.gif)|![Example 1 Prediction](data/q33/50_point.gif)|\n",
    "|![Example 2 Image](data/q33/gt_img_450.png)|![Example 2 Mesh](data/q33/gt_mesh_450.gif)|![Example 2 Prediction](data/q33/450_point.gif)|\n",
    "|![Example 3 Image](data/q33/gt_img_1250.png)|![Example 3 Mesh](data/q33/gt_mesh_1250.gif)|![Example 3 Prediction](data/q33/1250_point.gif)|\n",
    "\n",
    "\n",
    "Evaluation Results\n",
    "\n",
    "![Example 3 Image](data/q33/eval_point.png)\n",
    "\n",
    "Comparing training on one class versus three classes, quantitative results showed similar outcomes, but with three classes requiring significantly higher training and inference times. Qualitatively, visual results were also comparable, indicating that while multiple classes may offer more robustness, single-class training can be more computationally efficient. Overall, the decision depends on the trade-off between model robustness and computational resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit \n",
    "\n",
    "I attempted the implicit reconstruction with Neural Networks. These are the results I go\n",
    "|Implicit|![Implicit Source](data/q31/gt_img_0.png)|![IMplicit Mesh Target](data/q31/gt_mesh_0.gif)|![IMplicit Mesh Target](data/q31/0_implicit.gif)|\n",
    "---|---|---|---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
